{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4e7dcdde",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-10-27T17:33:44.263488Z",
     "iopub.status.busy": "2025-10-27T17:33:44.263132Z",
     "iopub.status.idle": "2025-10-27T17:33:46.469278Z",
     "shell.execute_reply": "2025-10-27T17:33:46.468237Z"
    },
    "papermill": {
     "duration": 2.211872,
     "end_time": "2025-10-27T17:33:46.471000",
     "exception": false,
     "start_time": "2025-10-27T17:33:44.259128",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/input/nbaiot-dataset/7.gafgyt.combo.csv\n",
      "/kaggle/input/nbaiot-dataset/9.gafgyt.combo.csv\n",
      "/kaggle/input/nbaiot-dataset/5.gafgyt.combo.csv\n",
      "/kaggle/input/nbaiot-dataset/1.mirai.udp.csv\n",
      "/kaggle/input/nbaiot-dataset/4.gafgyt.udp.csv\n",
      "/kaggle/input/nbaiot-dataset/6.gafgyt.udp.csv\n",
      "/kaggle/input/nbaiot-dataset/6.gafgyt.junk.csv\n",
      "/kaggle/input/nbaiot-dataset/data_summary.csv\n",
      "/kaggle/input/nbaiot-dataset/5.gafgyt.udp.csv\n",
      "/kaggle/input/nbaiot-dataset/9.gafgyt.junk.csv\n",
      "/kaggle/input/nbaiot-dataset/9.mirai.scan.csv\n",
      "/kaggle/input/nbaiot-dataset/1.benign.csv\n",
      "/kaggle/input/nbaiot-dataset/2.mirai.udpplain.csv\n",
      "/kaggle/input/nbaiot-dataset/3.gafgyt.combo.csv\n",
      "/kaggle/input/nbaiot-dataset/4.gafgyt.combo.csv\n",
      "/kaggle/input/nbaiot-dataset/6.mirai.scan.csv\n",
      "/kaggle/input/nbaiot-dataset/5.mirai.udp.csv\n",
      "/kaggle/input/nbaiot-dataset/3.benign.csv\n",
      "/kaggle/input/nbaiot-dataset/3.gafgyt.junk.csv\n",
      "/kaggle/input/nbaiot-dataset/7.gafgyt.scan.csv\n",
      "/kaggle/input/nbaiot-dataset/features.csv\n",
      "/kaggle/input/nbaiot-dataset/6.mirai.udp.csv\n",
      "/kaggle/input/nbaiot-dataset/6.gafgyt.tcp.csv\n",
      "/kaggle/input/nbaiot-dataset/6.gafgyt.combo.csv\n",
      "/kaggle/input/nbaiot-dataset/README.md\n",
      "/kaggle/input/nbaiot-dataset/6.mirai.syn.csv\n",
      "/kaggle/input/nbaiot-dataset/7.gafgyt.udp.csv\n",
      "/kaggle/input/nbaiot-dataset/5.gafgyt.junk.csv\n",
      "/kaggle/input/nbaiot-dataset/8.gafgyt.combo.csv\n",
      "/kaggle/input/nbaiot-dataset/7.gafgyt.junk.csv\n",
      "/kaggle/input/nbaiot-dataset/5.mirai.ack.csv\n",
      "/kaggle/input/nbaiot-dataset/4.mirai.syn.csv\n",
      "/kaggle/input/nbaiot-dataset/2.gafgyt.tcp.csv\n",
      "/kaggle/input/nbaiot-dataset/6.mirai.ack.csv\n",
      "/kaggle/input/nbaiot-dataset/2.mirai.ack.csv\n",
      "/kaggle/input/nbaiot-dataset/1.gafgyt.combo.csv\n",
      "/kaggle/input/nbaiot-dataset/6.mirai.udpplain.csv\n",
      "/kaggle/input/nbaiot-dataset/4.benign.csv\n",
      "/kaggle/input/nbaiot-dataset/2.gafgyt.scan.csv\n",
      "/kaggle/input/nbaiot-dataset/6.benign.csv\n",
      "/kaggle/input/nbaiot-dataset/4.mirai.udp.csv\n",
      "/kaggle/input/nbaiot-dataset/9.mirai.syn.csv\n",
      "/kaggle/input/nbaiot-dataset/9.mirai.udp.csv\n",
      "/kaggle/input/nbaiot-dataset/device_info.csv\n",
      "/kaggle/input/nbaiot-dataset/2.gafgyt.combo.csv\n",
      "/kaggle/input/nbaiot-dataset/5.gafgyt.scan.csv\n",
      "/kaggle/input/nbaiot-dataset/6.gafgyt.scan.csv\n",
      "/kaggle/input/nbaiot-dataset/9.mirai.udpplain.csv\n",
      "/kaggle/input/nbaiot-dataset/1.gafgyt.udp.csv\n",
      "/kaggle/input/nbaiot-dataset/9.gafgyt.scan.csv\n",
      "/kaggle/input/nbaiot-dataset/2.mirai.scan.csv\n",
      "/kaggle/input/nbaiot-dataset/8.gafgyt.scan.csv\n",
      "/kaggle/input/nbaiot-dataset/5.gafgyt.tcp.csv\n",
      "/kaggle/input/nbaiot-dataset/5.mirai.scan.csv\n",
      "/kaggle/input/nbaiot-dataset/5.benign.csv\n",
      "/kaggle/input/nbaiot-dataset/4.mirai.scan.csv\n",
      "/kaggle/input/nbaiot-dataset/9.benign.csv\n",
      "/kaggle/input/nbaiot-dataset/2.gafgyt.udp.csv\n",
      "/kaggle/input/nbaiot-dataset/1.mirai.udpplain.csv\n",
      "/kaggle/input/nbaiot-dataset/2.mirai.syn.csv\n",
      "/kaggle/input/nbaiot-dataset/3.gafgyt.scan.csv\n",
      "/kaggle/input/nbaiot-dataset/2.gafgyt.junk.csv\n",
      "/kaggle/input/nbaiot-dataset/4.gafgyt.junk.csv\n",
      "/kaggle/input/nbaiot-dataset/4.gafgyt.scan.csv\n",
      "/kaggle/input/nbaiot-dataset/8.benign.csv\n",
      "/kaggle/input/nbaiot-dataset/5.mirai.syn.csv\n",
      "/kaggle/input/nbaiot-dataset/5.mirai.udpplain.csv\n",
      "/kaggle/input/nbaiot-dataset/7.gafgyt.tcp.csv\n",
      "/kaggle/input/nbaiot-dataset/8.mirai.scan.csv\n",
      "/kaggle/input/nbaiot-dataset/1.gafgyt.tcp.csv\n",
      "/kaggle/input/nbaiot-dataset/1.mirai.ack.csv\n",
      "/kaggle/input/nbaiot-dataset/8.gafgyt.tcp.csv\n",
      "/kaggle/input/nbaiot-dataset/9.gafgyt.udp.csv\n",
      "/kaggle/input/nbaiot-dataset/4.mirai.udpplain.csv\n",
      "/kaggle/input/nbaiot-dataset/9.gafgyt.tcp.csv\n",
      "/kaggle/input/nbaiot-dataset/1.mirai.scan.csv\n",
      "/kaggle/input/nbaiot-dataset/8.gafgyt.junk.csv\n",
      "/kaggle/input/nbaiot-dataset/9.mirai.ack.csv\n",
      "/kaggle/input/nbaiot-dataset/4.gafgyt.tcp.csv\n",
      "/kaggle/input/nbaiot-dataset/8.mirai.udp.csv\n",
      "/kaggle/input/nbaiot-dataset/4.mirai.ack.csv\n",
      "/kaggle/input/nbaiot-dataset/2.mirai.udp.csv\n",
      "/kaggle/input/nbaiot-dataset/8.mirai.udpplain.csv\n",
      "/kaggle/input/nbaiot-dataset/2.benign.csv\n",
      "/kaggle/input/nbaiot-dataset/8.gafgyt.udp.csv\n",
      "/kaggle/input/nbaiot-dataset/1.gafgyt.scan.csv\n",
      "/kaggle/input/nbaiot-dataset/3.gafgyt.udp.csv\n",
      "/kaggle/input/nbaiot-dataset/1.gafgyt.junk.csv\n",
      "/kaggle/input/nbaiot-dataset/1.mirai.syn.csv\n",
      "/kaggle/input/nbaiot-dataset/3.gafgyt.tcp.csv\n",
      "/kaggle/input/nbaiot-dataset/8.mirai.ack.csv\n",
      "/kaggle/input/nbaiot-dataset/7.benign.csv\n",
      "/kaggle/input/nbaiot-dataset/8.mirai.syn.csv\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a09d8b52",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-27T17:33:46.477377Z",
     "iopub.status.busy": "2025-10-27T17:33:46.476977Z",
     "iopub.status.idle": "2025-10-27T17:33:47.865969Z",
     "shell.execute_reply": "2025-10-27T17:33:47.865025Z"
    },
    "papermill": {
     "duration": 1.39358,
     "end_time": "2025-10-27T17:33:47.867335",
     "exception": false,
     "start_time": "2025-10-27T17:33:46.473755",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setup Complete. Ready for Step 1: Merging.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# --- Configuration Constants ---\n",
    "# NOTE: Update this path if your dataset is named differently.\n",
    "BASE_PATH = '../input/nbaiot-dataset/' \n",
    "RANDOM_SEED = 42         # For reproducible results\n",
    "SAMPLE_SIZE = 10000      # Your requested random sample size\n",
    "\n",
    "# List to hold DataFrames from all files\n",
    "all_data = []\n",
    "\n",
    "print(\"Setup Complete. Ready for Step 1: Merging.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f9c5adbf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-27T17:33:47.873858Z",
     "iopub.status.busy": "2025-10-27T17:33:47.873086Z",
     "iopub.status.idle": "2025-10-27T17:40:25.972082Z",
     "shell.execute_reply": "2025-10-27T17:40:25.970414Z"
    },
    "papermill": {
     "duration": 398.110788,
     "end_time": "2025-10-27T17:40:25.980639",
     "exception": false,
     "start_time": "2025-10-27T17:33:47.869851",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1: Merging Data and Creating Labels...\n",
      "✅ Files merged successfully. Total rows: 7062819\n"
     ]
    }
   ],
   "source": [
    "# ==============================================================================\n",
    "# 1. Merge & Labeling (Using Library Functions, No Manual Loops)\n",
    "# ==============================================================================\n",
    "\n",
    "import glob\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "print(\"Step 1: Merging Data and Creating Labels...\")\n",
    "\n",
    "# Get all CSV file paths (recursive search)\n",
    "file_paths = glob.glob(os.path.join(BASE_PATH, \"**\", \"*.csv\"), recursive=True)\n",
    "\n",
    "if not file_paths:\n",
    "    raise FileNotFoundError(\"No CSV files found. Check BASE_PATH.\")\n",
    "\n",
    "# Read all CSV files into list of DataFrames\n",
    "df_list = [pd.read_csv(fp) for fp in file_paths]\n",
    "\n",
    "# Concatenate all data into one DataFrame\n",
    "df_raw = pd.concat(df_list, ignore_index=True)\n",
    "\n",
    "# Create label based on filename (vectorized)\n",
    "df_raw['label'] = [\n",
    "    0 if 'benign' in os.path.basename(fp).lower() else 1\n",
    "    for fp in file_paths\n",
    "    for _ in range(len(pd.read_csv(fp)))\n",
    "]\n",
    "\n",
    "print(f\"✅ Files merged successfully. Total rows: {len(df_raw)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c7c25146",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-27T17:40:25.989836Z",
     "iopub.status.busy": "2025-10-27T17:40:25.989348Z",
     "iopub.status.idle": "2025-10-27T17:40:26.435215Z",
     "shell.execute_reply": "2025-10-27T17:40:26.433908Z"
    },
    "papermill": {
     "duration": 0.452594,
     "end_time": "2025-10-27T17:40:26.436862",
     "exception": false,
     "start_time": "2025-10-27T17:40:25.984268",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Step 2: Sampling the Dataset...\n",
      "✅ Data sampled successfully: 10000 rows.\n"
     ]
    }
   ],
   "source": [
    "# ==============================================================================\n",
    "# 2. Sampling: Extract 10,000 Rows (Using Library Function - No Loops)\n",
    "# ==============================================================================\n",
    "\n",
    "from sklearn.utils import resample   # ✅ Library Sampling Function\n",
    "\n",
    "print(\"\\nStep 2: Sampling the Dataset...\")\n",
    "\n",
    "# SAMPLE_SIZE and RANDOM_SEED must be already defined.\n",
    "# Example:\n",
    "# SAMPLE_SIZE = 10000\n",
    "# RANDOM_SEED = 42\n",
    "\n",
    "# If dataset is smaller than sample size, resample will return all rows automatically\n",
    "df_sampled = resample(\n",
    "    df_raw,\n",
    "    replace=False,           # Do NOT duplicate rows\n",
    "    n_samples=min(SAMPLE_SIZE, len(df_raw)),  # Auto-adjust sample size\n",
    "    random_state=RANDOM_SEED\n",
    ")\n",
    "\n",
    "print(f\"✅ Data sampled successfully: {len(df_sampled)} rows.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bad1bfd4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-27T17:40:26.444324Z",
     "iopub.status.busy": "2025-10-27T17:40:26.443529Z",
     "iopub.status.idle": "2025-10-27T17:40:26.492489Z",
     "shell.execute_reply": "2025-10-27T17:40:26.491370Z"
    },
    "papermill": {
     "duration": 0.054236,
     "end_time": "2025-10-27T17:40:26.493991",
     "exception": false,
     "start_time": "2025-10-27T17:40:26.439755",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Step 3: Handling Special Values (Inf and NaN)...\n",
      "✅ Special values handled successfully.\n",
      "Data is now separated into:\n",
      "  X (Features): (10000, 122)\n",
      "  Y (Labels): (10000,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_13/1021418769.py:11: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df_sampled = df_sampled.replace([np.inf, -np.inf], [1e9, -1e9])\n"
     ]
    }
   ],
   "source": [
    "# ==============================================================================\n",
    "# 3. Data Cleaning: Handle Special Numerical Values (Inf and NaN) - Corrected\n",
    "# ==============================================================================\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "print(\"\\nStep 3: Handling Special Values (Inf and NaN)...\")\n",
    "\n",
    "# Replace positive and negative infinite values with large numbers\n",
    "df_sampled = df_sampled.replace([np.inf, -np.inf], [1e9, -1e9])\n",
    "\n",
    "# Fill NaN values with 0 safely using pandas function\n",
    "df_sampled = df_sampled.fillna(0)\n",
    "\n",
    "# Optional: ensure correct dtypes to avoid future warnings\n",
    "df_sampled = df_sampled.infer_objects()\n",
    "\n",
    "# Separate features (X) and target (Y)\n",
    "Y = df_sampled['label']\n",
    "X = df_sampled.drop(columns=['label'])\n",
    "\n",
    "print(\"✅ Special values handled successfully.\")\n",
    "print(\"Data is now separated into:\")\n",
    "print(f\"  X (Features): {X.shape}\")\n",
    "print(f\"  Y (Labels): {Y.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "59e521c4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-27T17:40:26.501175Z",
     "iopub.status.busy": "2025-10-27T17:40:26.500327Z",
     "iopub.status.idle": "2025-10-27T17:40:26.536158Z",
     "shell.execute_reply": "2025-10-27T17:40:26.535054Z"
    },
    "papermill": {
     "duration": 0.041077,
     "end_time": "2025-10-27T17:40:26.537861",
     "exception": false,
     "start_time": "2025-10-27T17:40:26.496784",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Step 4: Applying Min-Max Scaling (Normalization)...\n",
      "✅ Min-Max Scaling Complete.\n",
      "Final Preprocessed Features (X_scaled_df): (10000, 122)\n"
     ]
    }
   ],
   "source": [
    "# ==============================================================================\n",
    "# 4. Feature Scaling: Apply Min-Max Normalization (Using Library)\n",
    "# ==============================================================================\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler   # ✅ Library Used\n",
    "import pandas as pd                              # ✅ Needed to convert back to DataFrame\n",
    "\n",
    "print(\"\\nStep 4: Applying Min-Max Scaling (Normalization)...\")\n",
    "\n",
    "# Initialize Min-Max Scaler\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "# Fit the scaler on the feature data and transform it\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Convert scaled array back to DataFrame\n",
    "X_scaled_df = pd.DataFrame(X_scaled, columns=X.columns)\n",
    "\n",
    "print(\"✅ Min-Max Scaling Complete.\")\n",
    "print(f\"Final Preprocessed Features (X_scaled_df): {X_scaled_df.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ea8f12a2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-27T17:40:26.545117Z",
     "iopub.status.busy": "2025-10-27T17:40:26.544794Z",
     "iopub.status.idle": "2025-10-27T17:40:26.566417Z",
     "shell.execute_reply": "2025-10-27T17:40:26.565312Z"
    },
    "papermill": {
     "duration": 0.027061,
     "end_time": "2025-10-27T17:40:26.567960",
     "exception": false,
     "start_time": "2025-10-27T17:40:26.540899",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Step 5: Splitting Data for Training and Testing...\n",
      "✅ Full Preprocessing Pipeline Complete.\n",
      "  Training Features (X_train): (7000, 122)\n",
      "  Testing Features (X_test): (3000, 122)\n",
      "You can now proceed to Model Training using X_train and Y_train.\n"
     ]
    }
   ],
   "source": [
    "# ==============================================================================\n",
    "# 5. Final Split: Separate into Training and Testing Sets (Using Library)\n",
    "# ==============================================================================\n",
    "\n",
    "from sklearn.model_selection import train_test_split   # ✅ Library Used\n",
    "\n",
    "print(\"\\nStep 5: Splitting Data for Training and Testing...\")\n",
    "\n",
    "# Split the data using 70/30 ratio with stratification\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(\n",
    "    X_scaled_df,         # Features after scaling\n",
    "    Y,                   # Target labels\n",
    "    test_size=0.3,       # 30% data for testing\n",
    "    random_state=RANDOM_SEED,\n",
    "    stratify=Y           # ✅ Keeps anomaly ratio same in train & test\n",
    ")\n",
    "\n",
    "print(\"✅ Full Preprocessing Pipeline Complete.\")\n",
    "print(f\"  Training Features (X_train): {X_train.shape}\")\n",
    "print(f\"  Testing Features (X_test): {X_test.shape}\")\n",
    "print(\"You can now proceed to Model Training using X_train and Y_train.\")\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 480187,
     "sourceId": 897617,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31153,
   "isGpuEnabled": false,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 407.983715,
   "end_time": "2025-10-27T17:40:27.593916",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-10-27T17:33:39.610201",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
